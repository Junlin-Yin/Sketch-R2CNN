{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import skimage\n",
    "from skimage import io as skio\n",
    "from skimage import measure as skm\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TORCH_EXTENSIONS_DIR'] = '/tmp/torch_extensions_jypyter'\n",
    "\n",
    "_project_folder_ = os.path.realpath(os.path.abspath('..'))\n",
    "if _project_folder_ not in sys.path:\n",
    "    sys.path.insert(0, _project_folder_)\n",
    "from data.sketch_util import SketchUtil\n",
    "from neuralline.rasterize import Raster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Arguments\n",
    "quickdraw_root = '/media/hdd/craiglee/Data/sketchrnn/sketchrnn_data/'\n",
    "output_root = '/media/hdd/craiglee/Data/sketchrnn_processed_entropyfilter/'\n",
    "\n",
    "if not os.path.exists(output_root):\n",
    "    os.makedirs(output_root)\n",
    "\n",
    "img_size = 128\n",
    "thickness = 1.0\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('[*] Using device: {}'.format(device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_categories():\n",
    "    res = [npz_file.stem[:-5] for npz_file in list(Path(quickdraw_root).glob('*.full.npz'))]\n",
    "    return sorted(res, key=lambda s: s.lower())\n",
    "\n",
    "def load_npz(file_path):\n",
    "    npz = np.load(file_path, encoding='latin1')\n",
    "    return npz['train'], npz['valid'], npz['test']\n",
    "\n",
    "def cvrt_points3(points3_array):\n",
    "    # Make a copy and convert dtype\n",
    "    points3 = np.array(points3_array, dtype=np.int32)\n",
    "    points3[:, 0:2] = np.cumsum(points3[:, 0:2], axis=0)\n",
    "    return points3\n",
    "\n",
    "def cvrt_category_to_points3(points3_arrays, hdf5_group=None):\n",
    "    max_num_points = 0\n",
    "    res = []\n",
    "    for pts3_arr in points3_arrays:\n",
    "        if len(pts3_arr) < 3:\n",
    "            continue\n",
    "        pts3 = np.array(cvrt_points3(pts3_arr), np.float32)\n",
    "        pts3_norm = SketchUtil.normalization(pts3[:, 0:2])\n",
    "        if pts3_norm is None:\n",
    "            continue\n",
    "        pts3[:, 0:2] = pts3_norm\n",
    "        \n",
    "        npts3 = len(pts3)\n",
    "        if npts3 > max_num_points:\n",
    "            max_num_points = npts3\n",
    "        \n",
    "        if hdf5_group is not None:\n",
    "            hdf5_group.create_dataset(str(len(res)), data=pts3)\n",
    "\n",
    "        res.append(pts3)\n",
    "    return res, max_num_points\n",
    "\n",
    "def points3_to_img(points3_array):\n",
    "    points3_array_gpu = torch.from_numpy(points3_array).to(device)\n",
    "    img_gpu = Raster.to_image(torch.unsqueeze(points3_array_gpu, 0), \n",
    "                              1.0, img_size, thickness, device=device)\n",
    "    img_cpu = np.ascontiguousarray(img_gpu.cpu().numpy())\n",
    "    return np.array(255 * img_cpu[0, 0, :, :], np.uint8)\n",
    "\n",
    "def denoise_by_entropy(points3_arrays, hdf5_group=None):\n",
    "    # Ref:\n",
    "    # - SketchMate Deep Hashing for Million-Scale Human Sketch Retrieval\n",
    "    imgs = []\n",
    "    entropies = []\n",
    "    for pts3_arr in points3_arrays:\n",
    "        img = points3_to_img(pts3_arr)\n",
    "        imgs.append(img)\n",
    "        entropies.append(skm.shannon_entropy(img))\n",
    "\n",
    "    num_imgs = len(imgs)\n",
    "    range_low = int(num_imgs * 0.05)\n",
    "    range_high = int(num_imgs * 0.95)\n",
    "    \n",
    "    sort_indices = np.argsort(np.array(entropies)).tolist()\n",
    "\n",
    "    pts3_to_discard = []\n",
    "    imgs_to_discard = []\n",
    "    \n",
    "    nsketches = 0\n",
    "    max_num_points = 0\n",
    "    for idx, sidx in enumerate(sort_indices):\n",
    "        if range_low <= idx < range_high and hdf5_group is not None:\n",
    "            npts3 = len(points3_arrays[sidx])\n",
    "            if npts3 > max_num_points:\n",
    "                max_num_points = npts3\n",
    "\n",
    "            hdf5_group.create_dataset(str(nsketches), data=points3_arrays[sidx])\n",
    "            nsketches += 1\n",
    "        else:\n",
    "            pts3_to_discard.append(points3_arrays[sidx])\n",
    "            imgs_to_discard.append(imgs[sidx])\n",
    "    return nsketches, max_num_points, pts3_to_discard, imgs_to_discard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/35321093/limit-on-number-of-hdf5-datasets\n",
    "\n",
    "category_names = get_categories()\n",
    "print('[*] Number of categories = {}'.format(len(category_names)))\n",
    "print('[*] ------')\n",
    "print(category_names)\n",
    "print('[*] ------')\n",
    "\n",
    "hdf5_names = ['train', 'valid', 'test']\n",
    "mode_indices = [list() for hn in hdf5_names]\n",
    "hdf5_files = [h5py.File(os.path.join(output_root, 'quickdraw_{}.hdf5'.format(hn)), 'w', libver='latest') for hn in hdf5_names]\n",
    "hdf5_groups = [h5.create_group('/sketch') for h5 in hdf5_files]\n",
    "\n",
    "max_num_points = 0\n",
    "for cid, category_name in enumerate(category_names):\n",
    "    print('[*] Processing {}th category: {}'.format(cid + 1, category_name))\n",
    "\n",
    "    # Open the npz file\n",
    "    train_valid_test = load_npz(os.path.join(quickdraw_root, category_name + '.npz'))\n",
    "    \n",
    "    for mid, mode in enumerate(hdf5_names):\n",
    "        # Under a mode to create a group: train/valid/test\n",
    "        hdf5_category_group = hdf5_groups[mid].create_group(str(cid))\n",
    "\n",
    "        if True:\n",
    "            pts3_arrays, npts3 = cvrt_category_to_points3(train_valid_test[mid])\n",
    "\n",
    "            # Denoise\n",
    "            nsketches, npts3, pts3_discard, imgs_discard = denoise_by_entropy(pts3_arrays, hdf5_category_group)\n",
    "\n",
    "            discard_folder = os.path.join(output_root, 'discard', mode, category_name)\n",
    "            if not os.path.exists(discard_folder):\n",
    "                os.makedirs(discard_folder)\n",
    "            for img_id, img in enumerate(imgs_discard):\n",
    "                skio.imsave('{}/{}.png'.format(discard_folder, img_id), img)\n",
    "        else:\n",
    "            pts3_arrays, npts3 = cvrt_category_to_points3(train_valid_test[mid], hdf5_category_group)\n",
    "            nsketches = len(pts3_arrays)\n",
    "\n",
    "        if npts3 > max_num_points:\n",
    "            max_num_points = npts3\n",
    "\n",
    "        hdf5_category_group.attrs['num_sketches'] = nsketches\n",
    "        mode_indices[mid].extend(list(zip([cid] * nsketches, range(nsketches))))\n",
    "\n",
    "for gid, gp in enumerate(hdf5_groups):\n",
    "    gp.attrs['num_categories'] = len(category_names)\n",
    "    gp.attrs['max_points'] = max_num_points\n",
    "\n",
    "for hf in hdf5_files:\n",
    "    hf.flush()\n",
    "    hf.close()\n",
    "\n",
    "pkl_save = {'categories': category_names, 'indices': mode_indices}\n",
    "with open(os.path.join(output_root, 'categories.pkl'), 'wb') as fh:\n",
    "    pickle.dump(pkl_save, fh, pickle.HIGHEST_PROTOCOL)    \n",
    "\n",
    "print('max_num_points = {}'.format(max_num_points))\n",
    "print('All done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
